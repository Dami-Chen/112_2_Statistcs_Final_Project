{"cells":[{"cell_type":"code","execution_count":null,"id":"07712c1f","metadata":{"id":"07712c1f"},"outputs":[],"source":["#載入所需函示庫\n","import matplotlib\n","from matplotlib import pyplot as plt\n","%matplotlib inline\n","# 設定圖形大小; DPI越大圖越大\n","plt.rcParams[\"figure.dpi\"] = 100\n","import seaborn as sns\n","import pandas as pd\n","import numpy as np\n","import scipy.stats as stats\n","import statsmodels.api as sm\n","import statsmodels.stats.api as sms\n","import statsmodels.formula.api as smf\n","import statsmodels.stats.multicomp as smm\n","import statsmodels.stats.outliers_influence as sso\n","from statsmodels.stats.stattools import durbin_watson\n","import statsmodels\n","import statistics as stat\n","import statistics\n","import math\n","from sklearn.utils import shuffle\n","# !pip install stepwise-regression\n","from stepwise_regression import step_reg\n","import time\n","import itertools\n","from statsmodels.tsa.api import Holt"]},{"cell_type":"code","source":["!pip install stepwise-regression"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JpUPzjwodQXO","executionInfo":{"status":"ok","timestamp":1717826841228,"user_tz":-480,"elapsed":5678,"user":{"displayName":"morris chen","userId":"05337989253267718231"}},"outputId":"096cf056-6505-4232-8c2c-80a1b1243573"},"id":"JpUPzjwodQXO","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting stepwise-regression\n","  Downloading stepwise_regression-1.0.3-py3-none-any.whl (3.3 kB)\n","Requirement already satisfied: statsmodels in /usr/local/lib/python3.10/dist-packages (from stepwise-regression) (0.14.2)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stepwise-regression) (2.0.3)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->stepwise-regression) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stepwise-regression) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stepwise-regression) (2024.1)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas->stepwise-regression) (1.25.2)\n","Requirement already satisfied: scipy!=1.9.2,>=1.8 in /usr/local/lib/python3.10/dist-packages (from statsmodels->stepwise-regression) (1.11.4)\n","Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.10/dist-packages (from statsmodels->stepwise-regression) (0.5.6)\n","Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels->stepwise-regression) (24.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5.6->statsmodels->stepwise-regression) (1.16.0)\n","Installing collected packages: stepwise-regression\n","Successfully installed stepwise-regression-1.0.3\n"]}]},{"cell_type":"code","execution_count":null,"id":"651b1c31","metadata":{"id":"651b1c31"},"outputs":[],"source":["def my_inference(p_value, alpha, claim):\n","    if p_value < alpha:\n","        print(f\"Since the p_value = {p_value:.4f} < {alpha}, we reject the null hypothesis.\\nThat is, we have sufficient evidence to claim that {claim}.\")\n","    else:\n","        print(f\"Since the p_value = {p_value:.4f} > {alpha}, we do not reject the null hypothesis.\\nThat is, we do not have sufficient evidence to claim that {claim}.\")\n","\n","def my_normality_check(sample_data, data_name, data_unit, alpha):\n","    # histogram\n","    fig, ax = plt.subplots()\n","    counts, bins, patches = plt.hist((sample_data.dropna()).astype(float), 6, density=False, facecolor='g', alpha=0.75)\n","    plt.title(f'Histogram of {data_name}')\n","    plt.xlabel(data_unit)\n","    plt.ylabel('Frequency')\n","    plt.grid(True)\n","\n","    # Shapiro-Wilk test\n","    print(\"Shapiro-Wilk test for normality:\")\n","    print(\"H0: The distribution is normal.\")\n","    print(\"H1: The distribution is not normal.\")\n","    stat, p = stats.shapiro(sample_data.dropna())\n","    print(f'For population = {data_name}')\n","    print(f\"Shapiro statistic = {stat:.6f} and p_value = {p:.6f}\")\n","    p_value = p; claim = \"the distribution is not normal\"\n","    my_inference(p_value, alpha, claim)\n","\n","    # QQ plot\n","    fig = sm.qqplot(sample_data.dropna(), stats.norm, fit=True, line='45')\n","    plt.title(f'Q-Q Plot for {data_name}')\n","    plt.show()\n","\n","def my_scatter_plot(sample_data, X_name, Y_name, X_unit, Y_unit):\n","    _ = sns.regplot(x = sample_data[X_name], y = sample_data[Y_name], color = 'b', ci = None)\n","    plt.title(f'Scatter Plot for {X_name} and {Y_name}')\n","    plt.xlabel(f'{X_name} ({X_unit})')\n","    plt.ylabel(f'{Y_name} ({Y_unit})')\n","    plt.show()\n","\n","def my_scatter_plot_2(x, y):\n","    _ = sns.regplot(x = x, y = y, color = 'b', ci = None)\n","    plt.title(f'Scatter Plot for {x.name} and {y.name}')\n","    plt.xlabel(f'{x.name}')\n","    plt.ylabel(f'{y.name}')\n","    plt.show()\n","\n","def Sample_Mean_Hypothesis_Testing(x, H0_x_bar, a):\n","    x_bar = x.mean()\n","    x_std = stat.stdev(x)\n","    x_n = x.size\n","    print(f\"mean = {x_bar:.4f}\")\n","    print(f\"std. dev. = {x_std:.4f}\")\n","    print(f\"Number of observation = {x_n}\")\n","    print(f\"Hypothesized mean = {H0_x_bar}\")\n","    print(f\"Significant level = {a}\")\n","    tstat = (x_bar - H0_x_bar) / (x_std / (x_n ** 0.5))\n","    print(f\"t-stat = {tstat:.4f}\")\n","    if tstat > 0:\n","        tcv_onetail = stats.t.ppf(1 - a, df = x_n - 1)\n","    else:\n","        tcv_onetail = stats.t.ppf(a, df = x_n - 1)\n","    print(f\"t critical value one tail = {tcv_onetail:.4f}\")\n","    if tstat > 0:\n","        p_onetail = 1 - stats.t.cdf(tstat, df = x_n - 1)\n","    else:\n","        p_onetail = stats.t.cdf(tstat, df = x_n - 1)\n","    print(f\"p-value (one-tail) = {p_onetail:.4f}\")\n","    if tstat > 0:\n","        tcv_twotail = stats.t.ppf(1 - a/2, df = x_n - 1)\n","    else:\n","        tcv_twotail = stats.t.ppf(a/2, df = x_n - 1)\n","    print(f\"t critical value two tail = {tcv_twotail:.4f}\")\n","    p_twotail = p_onetail * 2\n","    print(f\"p-value (two-tail) = {p_twotail:.4f}\")\n","    return p_onetail, p_twotail\n","\n","def runsTest(l, l_median):\n","    runs, n1, n2 = 1, 0, 0\n","    if(l[0]) >= l_median:\n","        n1 += 1\n","    else:\n","        n2 += 1\n","    for i in range(1, len(l)):\n","        if (l[i] >= l_median and l[i-1] < l_median) or (l[i] < l_median and l[i-1] >= l_median):\n","            runs += 1\n","        if(l[i]) >= l_median:\n","            n1 += 1\n","        else:\n","            n2 += 1\n","    runs_exp = ((2*n1*n2)/(n1+n2)) + 1\n","    stan_dev = math.sqrt((2*n1*n2*(2*n1*n2-n1-n2))/(((n1+n2)**2)*(n1+n2-1)))\n","    z = (runs-runs_exp)/stan_dev\n","    pval_z = stats.norm.sf(abs(z)) * 2\n","    print('runs = ', runs)\n","    print('n1 = ', n1)\n","    print('n2 = ', n2)\n","    print('runs_exp = ', runs_exp)\n","    print('stan_dev = ', stan_dev)\n","    print('z = ', z)\n","    print('pval_z = ', pval_z)\n","    return pval_z\n","\n","def Durbin_Watson_test(x):\n","    x_square_sum = np.vdot(x, x)\n","    print(\"x_square_sum = \", x_square_sum)\n","    size = x.size\n","    print(\"size = \", size)\n","    x_d = np.zeros((size))\n","    print(\"x_d = \", x_d)\n","    l_size = size - 1\n","    for i in range(l_size):\n","        x_d[i + 1] = x[i + 1] - x[i]\n","    print(\"x_d = \", x_d)\n","    d = np.vdot(x_d, x_d) / x_square_sum\n","    print(\"d = \", d)\n","    return(d)\n","\n","def required_conditions_for_error(SD, y_pre, Y_name, d):\n","    # check required conditions\n","    print('Checking for required conditions for error variable:\\n')\n","\n","    mu = np.mean(SD)\n","    sigma = np.std(SD)\n","\n","    ## The error is a random variable with mean of zero.\n","    print('\\n1. Zero mean')\n","    print(f'H0: Errors have zero mean.')\n","    print(f'H1: Errors do not have zero mean.')\n","    H0_x_bar = 0\n","    p_onetail, p_twotail = Sample_Mean_Hypothesis_Testing(SD, H0_x_bar, alpha)\n","    claim = \"the errors do not have zero mean\"\n","    my_inference(p_twotail, alpha, claim)\n","\n","    print(f'\\n2. Normality')\n","    print(f'H0: Errors are normally distributed.')\n","    print(f'H1: Errors are not normally distributed.')\n","\n","\n","    ## The error is a normally distributed random variable.\n","    my_normality_check(pd.Series(SD), \"standarized residuals\", \"unit\", alpha)\n","\n","    ## The variance of the error term is the same for all values of the independent variable.\n","    print('\\n3. Homoskedasticity')\n","    print('H0: Homoskedasticity')\n","    print('H1: Heteroskedasticity')\n","\n","    plt.plot(y_pre, SD, 'o', color = 'gray')\n","    plt.axhline(y=0, color = 'blue')\n","    plt.axhline(y=2, color = 'red')\n","    plt.axhline(y=-2, color = 'red')\n","    plt.title('Standardized Residual Plot')\n","    plt.xlabel(Y_name)\n","    plt.ylabel('Standardized Residual')\n","    plt.show()\n","    print('Do not rejected H0. Heteroscedasticity does not appear to be a problem.')\n","\n","\n","    ## The values of the error are independent\n","    print('\\n4. Independence')\n","    print('\\n4-1. Randomness')\n","    print('H0 : Randomness exists.')\n","    print('H1 : Randomness does not exist.')\n","    SD_median = statistics.median(SD)\n","    Z_pval = runsTest(SD, SD_median)\n","    print('p_value for Z-statistic= ', Z_pval)\n","    claim = 'randomness does not exist'\n","    my_inference(Z_pval, alpha, claim)\n","\n","    print('\\n4-2. No Autocorrelation')\n","    print('H0 : There is no first-order correlation.')\n","    print('H1 : There is first-order correlation.')\n","#     float(durbin_watson(results.resid))\n","    print(f'd = {d:.4f}')\n","\n","def my_corr(y, X):\n","    data = pd.concat([y, X], axis=1)\n","    corr = data.corr()\n","    _ = sns.heatmap(corr, annot=True)\n","    multicollinearity_pairs = []\n","    for i in range(1, corr.shape[0]):\n","        for j in range(i + 1, corr.shape[0]):\n","            if (corr.iloc[i, j] > 0.7):\n","                multicollinearity_pairs.append((corr.columns[i], corr.columns[j]))\n","    corr_greater_than_07 = True\n","    if len(multicollinearity_pairs) == 0:\n","        print(\"No apparent multicollinearity problem of coefficients of correlation greater than 0.7.\")\n","        corr_greater_than_07 = False\n","    else:\n","        print(f\"Multicollinearity may exist between {multicollinearity_pairs} since their correlation of coefficient are greater than 0.7.\")\n","    return corr, corr_greater_than_07\n","\n","def my_outlier(SD):\n","    df = pd.DataFrame(SD,columns = ['SD'])\n","    filter = (df['SD'] < -2) | (df['SD'] > 2)\n","    outliers = df['SD'].loc[filter]\n","    if len(outliers) == 0:\n","        print('There are no outliers.')\n","    else:\n","        print(\"Outliers by SD = \\n\")\n","        print(outliers)\n","        print('\\nOutliers:')\n","        out = ''\n","        indices = outliers.index\n","        for i, idx in enumerate(indices):\n","            if i == len(indices) - 1:\n","                out += f'sample {idx + 1}.'\n","            else:\n","                out += f'sample {idx + 1}, '\n","        print(out)\n","\n","def my_hii(X, ols_result):\n","    H = np.matmul(X, np.linalg.solve(np.matmul(X.T, X), X.T))\n","    df_h = pd.DataFrame({\n","        'hii': np.diagonal(H)\n","    })\n","    k = ols_result.df_model\n","    n = len(df_h['hii'])\n","    h_level = 3 * (k + 1) / n\n","    print(\"h_level = \", h_level)\n","    print(\" \\n\")\n","    filter = (df_h['hii'] > h_level )\n","    hii = df_h['hii']\n","    inf_hii = hii.loc[filter]\n","    if len(inf_hii) == 0:\n","        print('There are no influential observations by hii.')\n","    else:\n","        print(\"Influential observations by hii = \\n\")\n","        print(inf_hii)\n","        print('\\Influential observations:')\n","        out = ''\n","        indices = inf_hii.index\n","        for i, idx in enumerate(indices):\n","            if i == len(indices) - 1:\n","                out += f'sample {idx + 1}.'\n","            else:\n","                out += f'sample {idx + 1}, '\n","    return hii, inf_hii\n","\n","\n","def my_cooks_distance(ols_result, data, hii):\n","    s2_e = ols_result.mse_resid\n","    k = ols_result.df_model\n","    y_a = data[:, 1]\n","    y_f = data[:, 2]\n","    CD_arr = np.square(y_a - y_f) / s2_e / (k - 1) * hii / np.square(1 - hii)\n","    CD = np.array(CD_arr)\n","    df_cd = pd.DataFrame(CD,columns = ['CD'])\n","    filter = (df_cd['CD'] > 1 )\n","    inf_cd = df_cd['CD'].loc[filter]\n","\n","    if len(inf_cd) == 0:\n","        print(\"There are no influential observations by Cook's Distances.\")\n","    else:\n","        print(\"Influential observations by Cook's Distances = \\n\")\n","        print(inf_cd)\n","        print('\\Influential observations:')\n","        out = ''\n","        indices = inf_cd.index\n","        for i, idx in enumerate(indices):\n","            if i == len(indices) - 1:\n","                out += f'sample {idx + 1}.'\n","            else:\n","                out += f'sample {idx + 1}, '\n","    return inf_cd\n","\n","def my_simple_linear_regression(sample_data, X_name, Y_name, X_unit, Y_unit, alpha):\n","    # scatter plot\n","    my_scatter_plot(sample_data, X_name, Y_name, X_unit, Y_unit)\n","\n","    # ols regression\n","    ols_result = smf.ols(f'{Y_name} ~ {X_name}', data = sample_data).fit()\n","    display(ols_result.summary())\n","    b1 = ols_result.params[1]\n","    b0 = ols_result.params[0]\n","    print(f\"Estimated model: {Y_name} = {b0:0.4f} + {b1:0.4f} {X_name}\")\n","\n","    ## interpret the model\n","    print(f'The intercept is b0 = {b0:0.4f}.')\n","    print(f'The slope of the line is b1 = {b1:0.4f}. For each additional {X_unit} on {X_name}, \\\n","          {Y_name} decreases by an average of {b1:0.4f} {Y_unit}.')\n","\n","\n","\n","    # check required conditions\n","\n","    ## required conditions for error term\n","    st, data, ss = sso.summary_table(ols_result, alpha = alpha)\n","    SD = data[:, 10]\n","    y_pre = data[:, 2]\n","\n","    required_conditions_for_error(SD, y_pre, Y_name)\n","\n","    # detecting outliers\n","    my_outlier(SD)\n","\n","    # influential observations\n","    ## hii\n","    hii, inf_hii = my_hii(sample_data.drop(sample_data.columns[0], axis = 1) , ols_result)\n","\n","    ## cook's distance\n","    my_cooks_distance(ols_result, data, hii)\n","\n","\n","\n","def my_multiple_linear_regression(y, X, alpha):\n","    ## no muliticollinearity\n","    multicollinearity_pairs = my_multicollinearity_test(y, X)\n","\n","    # ols regression\n","    X = sm.add_constant(X)\n","\n","    ols_result = sm.OLS(y, X).fit()\n","    print(ols_result.summary())\n","\n","    # check required conditions\n","\n","    ## required conditions for error term\n","    st, data, ss = sso.summary_table(ols_result, alpha = alpha)\n","    SD = data[:, 10]\n","    y_pre = data[:, 2]\n","    required_conditions_for_error(SD, y_pre, y.name)\n","\n","    # detecting outliers\n","    my_outlier(SD)\n","\n","    # influential observations\n","    ## hii\n","    hii, inf_hii = my_hii(X, ols_result)\n","\n","    ## cook's distance\n","    my_cooks_distance(ols_result, data, hii)\n","\n","\n","    print('\\nAssessing the model:')\n","\n","    ## The Standard Error of Estimate\n","    print('\\nAssessment 1: standard error of estimate')\n","    s2_e = ols_result.mse_resid\n","    print(f'MSE: {s2_e:.4f}')\n","    s_e = ols_result.mse_resid ** 0.5\n","    print(f'Standard errors: {s_e:.4f}')\n","    print(f\"mean of y = {y.mean():.4f}\")\n","    print(f\"variance of y = {y.var(ddof=1):.4f}\")\n","    print(f\"standard deviation of y = {y.std(ddof=1):.4f}\")\n","\n","    ## The Coefficient of Determination\n","    print('\\nAssessment 2: Coefficient of Determination')\n","    print(f\"R-squared: {ols_result.rsquared}\")\n","    print(f\"Adjusted R-squared: {ols_result.rsquared_adj}\")\n","    if abs(ols_result.rsquared - ols_result.rsquared_adj) > 0.06:\n","        print(f'Since the difference between R-squared and the adjusted R-squared is {abs(ols_result.rsquared - ols_result.rsquared_adj):.4f} > 0.06, there might be a problem of over-fitting.')\n","    else:\n","        print(f'Since the difference between R-squared and the adjusted R-squared is {abs(ols_result.rsquared - ols_result.rsquared_adj):.4f} <= 0.06, there will not be a problem of over-fitting.')\n","\n","    ## The F-test of ANOVA\n","    f_res = ols_result.fvalue\n","    print(\"F value = \", f_res)\n","    MSE = ols_result.mse_resid\n","    df_model = ols_result.df_model\n","    df_error = ols_result.df_resid\n","    MSR = f_res * MSE\n","    SSR = MSR * df_model\n","    print(\"SSR = \", SSR, \"df = \", df_model, \"MSR = \", MSR)\n","    print(\"SSE = \", MSE * df_error, \"df = \", df_error, \"MSE = \", MSE)\n","    print(\"F = \", MSR / MSE)\n","    A = np.identity(len(ols_result.params))\n","    A = A[1:,:]\n","    print(\"F test = \", ols_result.f_test(A))\n","    p_value = ols_result.f_pvalue\n","    claim = 'the model is valid'\n","    my_inference(p_value, alpha, claim)\n","    return ols_result\n","\n","\n","def processSubset(y_v, X_v, feature_set):\n","    X_v_a = sm.add_constant(X_v[list(feature_set)])\n","    model = sm.OLS(y_v,X_v_a)\n","    regr = model.fit()\n","    RSS = regr.rsquared_adj\n","    return { \"model\":regr.model.exog_names, \"RSS\":RSS }\n","\n","def getBest(y_g, X_g, k):\n","    tic = time.time()\n","    results = []\n","    for combo in itertools.combinations(X_g.columns, k):\n","        results.append(processSubset(y_g, X_g, combo))\n","    models = pd.DataFrame(results)\n","    best_model = models.loc[models['RSS'].argmax()]\n","    toc = time.time()\n","    return best_model"]},{"cell_type":"code","execution_count":null,"id":"c6d0df3a","metadata":{"id":"c6d0df3a"},"outputs":[],"source":["class LAZY_MLR:\n","    def __init__(self, y, X, alpha):\n","        self.y = y\n","        self.X = X\n","        self.alpha = 0.05\n","        X = sm.add_constant(X)\n","        self.ols_result = sm.OLS(y, X).fit()\n","#         print(self.ols_result.summary())\n","        st, data, ss = sso.summary_table(self.ols_result, alpha = alpha)\n","        self.SD = data[:, 10]\n","        self.y_pre = data[:, 2]\n","        self.durbin_watson_value = float(durbin_watson(self.ols_result.resid))\n","    def get_formula(self):\n","        s = f\"{y.name} = {self.ols_result.params[0]:.4f}\"\n","        for i in range(1, len(self.ols_result.params)):\n","            s += f\" + {self.ols_result.params[i]:.4f} {self.ols_result.params.index[i]}\"\n","        print(s)\n","        return s\n","\n","    def get_ols_result(self):\n","        print(self.ols_result.summary())\n","\n","    def get_scatter_plots(self):\n","        for col in self.X.columns:\n","            my_scatter_plot_2(self.X[col], self.y)\n","\n","    def get_corr(self):\n","        self.corr, self.corr_greater_than_07 = my_corr(self.y, self.X)\n","\n","    def check_multicollinearity(self):\n","        if not hasattr(self, \"corr\"):\n","            self.get_corr()\n","        opposite_signs = False\n","        for i in range(1, self.corr.shape[0]):\n","            if self.ols_result.params[i] * self.corr[y.name].iloc[i] < 0:\n","                opposite_signs = True\n","                print(f\"The coefficient of correlation of ({self.y.name}, {self.ols_result.params.index[i]}) is {self.corr.iloc[0, i]:.4f} and the estimated coeffiecient is {self.ols_result.params[i]:.4f}. They have opposite signs, which is probably because of the multicollinearity problems.\")\n","        if not opposite_signs:\n","            print('No opposite signs.')\n","        if not self.corr_greater_than_07 and not opposite_signs:\n","            print('Multicollinearity will not be a problem.')\n","        else:\n","            print('Multicollinearity may be a problem.')\n","\n","\n","    def forward_stepwise(self):\n","        self.forward_select = step_reg.forward_regression(self.X, self.y, self.alpha, verbose=False)\n","        print(self.forward_select)\n","\n","\n","    def backward_stepwise(self):\n","        self.backward_select = step_reg.backward_regression(self.X, self.y, self.alpha, verbose=False)\n","        print(self.backward_select)\n","\n","    def best_subset(self):\n","        models_best = pd.DataFrame(columns=[\"RSS\", \"model\"])\n","        tic = time.time()\n","        for i in range(1, int(self.ols_result.df_model) + 1):\n","            models_best.loc[i] = getBest(self.y, self.X, i)\n","        toc = time.time()\n","        print(\"Total elapsed time:\", (toc-tic), \"seconds.\")\n","        display(models_best)\n","#         Fb = models_best[models_best['RSS']==models_best.RSS.max()].index.values\n","#         print(models_best.loc[Fb[0], \"model\"].summary())\n","\n","    def check_required_conditions_for_error(self):\n","        required_conditions_for_error(self.SD, self.y_pre, self.y.name, self.durbin_watson_value)\n","\n","    def get_model_assessment(self):\n","        # assessing the model\n","        print('Assessing the model:\\n')\n","\n","        ## standard error of estimate\n","        print('Assessment 1: Standard error of estimate')\n","        s2_e = self.ols_result.mse_resid\n","        print(f'MSE: {s2_e:.4f}')\n","        s_e = self.ols_result.mse_resid ** 0.5\n","        print(f'Standard errors: {s_e:.4f}')\n","        print(f\"mean of y = {self.y.mean():.4f}\")\n","        print(f\"variance of y = {self.y.var(ddof=1):.4f}\")\n","        print(f\"standard deviation of y = {self.y.std(ddof=1):.4f}\")\n","        if s_e < 0.3 * self.y.mean():\n","            print(\"The standard errors are relatively small, indicating that the model's variation is small and the model is a good fit.\")\n","        else:\n","            print(\"The standard errors are relatively big, indicating that the model's variation is big and the model is not a good fit.\")\n","\n","        ## The Coefficient of Determination\n","        print('\\nAssessment 2: Coefficient of Determination')\n","        print(f\"R-squared: {self.ols_result.rsquared}\")\n","        print(f\"Adjusted R-squared: {self.ols_result.rsquared_adj}\")\n","        if abs(self.ols_result.rsquared - self.ols_result.rsquared_adj) > 0.06:\n","            print(f'Since the difference between R-squared and the adjusted R-squared is {abs(self.ols_result.rsquared - self.ols_result.rsquared_adj):.4f} > 0.06, there might be a problem of over-fitting.')\n","        else:\n","            print(f'Since the difference between R-squared and the adjusted R-squared is {abs(self.ols_result.rsquared - self.ols_result.rsquared_adj):.4f} <= 0.06, there will not be a problem of over-fitting.')\n","\n","        ## The F-test of ANOVA\n","        print('\\nAssessment 3: F-test of ANOVA')\n","        print(\"H0: All of the estimated coefficients are zero.\")\n","        print(\"H1: Not all of the estimated coefficients are zero.\")\n","        f_res = self.ols_result.fvalue\n","        print(f\"F value = {f_res:.4f}\")\n","        MSE = self.ols_result.mse_resid\n","        df_model = self.ols_result.df_model\n","        df_error = self.ols_result.df_resid\n","        MSR = f_res * MSE\n","        SSR = MSR * df_model\n","        print(\"SSR = \", SSR, \"df = \", df_model, \"MSR = \", MSR)\n","        print(\"SSE = \", MSE * df_error, \"df = \", df_error, \"MSE = \", MSE)\n","        print(\"F = \", MSR / MSE)\n","        A = np.identity(len(self.ols_result.params))\n","        A = A[1:,:]\n","        print(\"F test = \", self.ols_result.f_test(A))\n","        p_value = self.ols_result.f_pvalue\n","        claim = 'the model is valid'\n","        my_inference(p_value, alpha, claim)\n","\n","        # Testing of the Coefficients\n","        print('\\nAssessment 4: t-test of the coefficients')\n","        for i in range(1, int(self.ols_result.df_model) + 1):\n","            print(f\"H0: beta {i} = 0\")\n","            print(f\"H1: beta {i} ≠ 0\")\n","            claim = f\"the coefficient of {self.ols_result.params.index[i]} is not 0\"\n","            my_inference(self.ols_result.pvalues[i], self.alpha, claim)\n","\n","class LAZY_SLR:\n","    def __init__(self, y, x, alpha):\n","        self.y = y\n","        self.x = x\n","        self.alpha = 0.05\n","        x = sm.add_constant(x)\n","        self.ols_result = sm.OLS(y, x).fit()\n","#         print(self.ols_result.summary())\n","        st, data, ss = sso.summary_table(self.ols_result, alpha = alpha)\n","        self.SD = data[:, 10]\n","        self.y_pre = data[:, 2]\n","        self.durbin_watson_value = float(durbin_watson(self.ols_result.resid))\n","    def get_formula(self):\n","        s = f\"{y.name} = {self.ols_result.params[0]:.4f}\"\n","        for i in range(1, len(self.ols_result.params)):\n","            s += f\" + {self.ols_result.params[i]:.4f} {self.ols_result.params.index[i]}\"\n","        print(s)\n","        return s\n","\n","    def get_ols_result(self):\n","        print(self.ols_result.summary())\n","\n","    def get_scatter_plots(self):\n","        my_scatter_plot_2(self.x, self.y)\n","\n","    def get_corr(self):\n","        self.corr, self.corr_greater_than_07 = my_corr(self.y, self.x)\n","\n","#     def check_multicollinearity(self):\n","#         if not hasattr(self, \"corr\"):\n","#             self.get_corr()\n","#         opposite_signs = False\n","#         for i in range(1, self.corr.shape[0]):\n","#             if self.ols_result.params[i] * self.corr[y.name].iloc[i] < 0:\n","#                 opposite_signs = True\n","#                 print(f\"The coefficient of correlation of ({self.y.name}, {self.ols_result.params.index[i]}) is {self.corr.iloc[0, i]:.4f} and the estimated coeffiecient is {self.ols_result.params[i]:.4f}. They have opposite signs, which is probably because of the multicollinearity problems.\")\n","#         if not opposite_signs:\n","#             print('No opposite signs.')\n","#         if not self.corr_greater_than_07 and not opposite_signs:\n","#             print('Multicollinearity will not be a problem.')\n","#         else:\n","#             print('Multicollinearity may be a problem.')\n","\n","\n","#     def forward_stepwise(self):\n","#         self.forward_select = step_reg.forward_regression(self.x, self.y, self.alpha, verbose=False)\n","#         print(self.forward_select)\n","\n","\n","#     def backward_stepwise(self):\n","#         self.backward_select = step_reg.backward_regression(self.x, self.y, self.alpha, verbose=False)\n","#         print(self.backward_select)\n","\n","#     def best_subset(self):\n","#         models_best = pd.DataFrame(columns=[\"RSS\", \"model\"])\n","#         tic = time.time()\n","#         for i in range(1, int(self.ols_result.df_model) + 1):\n","#             models_best.loc[i] = getBest(self.y, self.X, i)\n","#         toc = time.time()\n","#         print(\"Total elapsed time:\", (toc-tic), \"seconds.\")\n","#         display(models_best)\n","# #         Fb = models_best[models_best['RSS']==models_best.RSS.max()].index.values\n","# #         print(models_best.loc[Fb[0], \"model\"].summary())\n","\n","    def check_required_conditions_for_error(self):\n","        required_conditions_for_error(self.SD, self.y_pre, self.y.name, self.durbin_watson_value)\n","\n","    def get_model_assessment(self):\n","        # assessing the model\n","        print('Assessing the model:\\n')\n","\n","        ## standard error of estimate\n","        print('Assessment 1: Standard error of estimate')\n","        s2_e = self.ols_result.mse_resid\n","        print(f'MSE: {s2_e:.4f}')\n","        s_e = self.ols_result.mse_resid ** 0.5\n","        print(f'Standard errors: {s_e:.4f}')\n","        print(f\"mean of y = {self.y.mean():.4f}\")\n","        print(f\"variance of y = {self.y.var(ddof=1):.4f}\")\n","        print(f\"standard deviation of y = {self.y.std(ddof=1):.4f}\")\n","        if s_e < 0.3 * self.y.mean():\n","            print(\"The standard errors are relatively small, indicating that the model's variation is small and the model is a good fit.\")\n","        else:\n","            print(\"The standard errors are relatively big, indicating that the model's variation is big and the model is not a good fit.\")\n","\n","        ## The Coefficient of Determination\n","        print('\\nAssessment 2: Coefficient of Determination')\n","        print(f\"R-squared: {self.ols_result.rsquared}\")\n","        print(f\"Adjusted R-squared: {self.ols_result.rsquared_adj}\")\n","        if abs(self.ols_result.rsquared - self.ols_result.rsquared_adj) > 0.06:\n","            print(f'Since the difference between R-squared and the adjusted R-squared is {abs(self.ols_result.rsquared - self.ols_result.rsquared_adj):.4f} > 0.06, there might be a problem of over-fitting.')\n","        else:\n","            print(f'Since the difference between R-squared and the adjusted R-squared is {abs(self.ols_result.rsquared - self.ols_result.rsquared_adj):.4f} <= 0.06, there will not be a problem of over-fitting.')\n","\n","        ## The F-test of ANOVA\n","        print('\\nAssessment 3: F-test of ANOVA')\n","        print(\"H0: All of the estimated coefficients are zero.\")\n","        print(\"H1: Not all of the estimated coefficients are zero.\")\n","        f_res = self.ols_result.fvalue\n","        print(f\"F value = {f_res:.4f}\")\n","        MSE = self.ols_result.mse_resid\n","        df_model = self.ols_result.df_model\n","        df_error = self.ols_result.df_resid\n","        MSR = f_res * MSE\n","        SSR = MSR * df_model\n","        print(\"SSR = \", SSR, \"df = \", df_model, \"MSR = \", MSR)\n","        print(\"SSE = \", MSE * df_error, \"df = \", df_error, \"MSE = \", MSE)\n","        print(\"F = \", MSR / MSE)\n","        A = np.identity(len(self.ols_result.params))\n","        A = A[1:,:]\n","        print(\"F test = \", self.ols_result.f_test(A))\n","        p_value = self.ols_result.f_pvalue\n","        claim = 'the model is valid'\n","        my_inference(p_value, alpha, claim)\n","\n","        # Testing of the Coefficients\n","        print('\\nAssessment 4: t-test of the coefficients')\n","        for i in range(1, int(self.ols_result.df_model) + 1):\n","            print(f\"H0: beta {i} = 0\")\n","            print(f\"H1: beta {i} ≠ 0\")\n","            claim = f\"the coefficient of {self.ols_result.params.index[i]} is not 0\"\n","            my_inference(self.ols_result.pvalues[i], self.alpha, claim)"]},{"cell_type":"code","source":["!gdown 100cvZSLmcYaF6hyRBASBPQI1LSxsb6RP"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XDG7CzuIdiV4","executionInfo":{"status":"ok","timestamp":1717826925646,"user_tz":-480,"elapsed":3457,"user":{"displayName":"morris chen","userId":"05337989253267718231"}},"outputId":"60bfd134-1f3d-445a-c10f-7cf8f152c139"},"id":"XDG7CzuIdiV4","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading...\n","From: https://drive.google.com/uc?id=100cvZSLmcYaF6hyRBASBPQI1LSxsb6RP\n","To: /content/nba_player_stats_v2.csv\n","\r  0% 0.00/145k [00:00<?, ?B/s]\r100% 145k/145k [00:00<00:00, 108MB/s]\n"]}]},{"cell_type":"code","execution_count":null,"id":"4cab5ce4","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"id":"4cab5ce4","executionInfo":{"status":"ok","timestamp":1717826941674,"user_tz":-480,"elapsed":570,"user":{"displayName":"morris chen","userId":"05337989253267718231"}},"outputId":"14cbcbac-2730-422f-8582-ce2e4108a440"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["              Player Pos  Age   Tm   G  GS    MP   FG  FGA    FG%  ...  \\\n","0        Jett Howard  SF   20  ORL  18   0   3.7  0.6  1.7  0.333  ...   \n","1    Tosan Evbuomwan  SF   22  DET  17   8  21.6  2.1  4.2  0.507  ...   \n","2       Jared Butler  SG   23  WAS  40   0  14.2  2.5  5.0  0.488  ...   \n","3      Grayson Allen  SG   28  PHO  75  74  33.5  4.5  9.1  0.499  ...   \n","4       Daniel Theis   C   31  LAC  60   3  16.9  2.6  4.9  0.532  ...   \n","..               ...  ..  ...  ...  ..  ..   ...  ...  ...    ...  ...   \n","489       Kevin Knox  SF   24  DET  31  11  18.1  2.8  6.0  0.462  ...   \n","490        KJ Martin  SF   23  PHI  60   2  12.4  1.6  3.0  0.536  ...   \n","491        Matt Ryan  SF   26  NOP  28   1  13.9  1.8  4.0  0.434  ...   \n","492       JD Davison  SG   21  BOS   8   0   4.9  0.6  1.5  0.417  ...   \n","493       Jack White  SF   26  MEM   4   0  16.0  0.5  4.0  0.125  ...   \n","\n","     StateLevel  Level1  Level2  PosCtg  Guard  Forward    Salary   lgSalary  \\\n","0             1       1       0       F      0        1  50.26824  15.430299   \n","1             2       0       1       F      0        1   2.95977  12.598037   \n","2             3       0       0       G      1        0  18.09782  14.408717   \n","3             2       0       1       G      1        0  89.25000  16.004367   \n","4             1       1       0       C      0        0  91.08387  16.024706   \n","..          ...     ...     ...     ...    ...      ...       ...        ...   \n","489           2       0       1       F      0        1  21.44320  14.578333   \n","490           1       1       0       F      0        1  19.30681  14.473383   \n","491           3       0       0       F      0        1  20.59782  14.538111   \n","492           2       0       1       G      1        0   5.59782  13.235303   \n","493           2       0       1       F      0        1   7.03550  13.463894   \n","\n","      gs_rate      Core  \n","0    0.000000  0.000000  \n","1    0.470588  0.470588  \n","2    0.000000  0.000000  \n","3    0.986667  0.986667  \n","4    0.050000  0.050000  \n","..        ...       ...  \n","489  0.354839  0.354839  \n","490  0.033333  0.033333  \n","491  0.035714  0.035714  \n","492  0.000000  0.000000  \n","493  0.000000  0.000000  \n","\n","[494 rows x 60 columns]"],"text/html":["\n","  <div id=\"df-8bd4f884-8494-4964-8fa9-ae33dfc3e8e3\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Player</th>\n","      <th>Pos</th>\n","      <th>Age</th>\n","      <th>Tm</th>\n","      <th>G</th>\n","      <th>GS</th>\n","      <th>MP</th>\n","      <th>FG</th>\n","      <th>FGA</th>\n","      <th>FG%</th>\n","      <th>...</th>\n","      <th>StateLevel</th>\n","      <th>Level1</th>\n","      <th>Level2</th>\n","      <th>PosCtg</th>\n","      <th>Guard</th>\n","      <th>Forward</th>\n","      <th>Salary</th>\n","      <th>lgSalary</th>\n","      <th>gs_rate</th>\n","      <th>Core</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Jett Howard</td>\n","      <td>SF</td>\n","      <td>20</td>\n","      <td>ORL</td>\n","      <td>18</td>\n","      <td>0</td>\n","      <td>3.7</td>\n","      <td>0.6</td>\n","      <td>1.7</td>\n","      <td>0.333</td>\n","      <td>...</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>F</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>50.26824</td>\n","      <td>15.430299</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Tosan Evbuomwan</td>\n","      <td>SF</td>\n","      <td>22</td>\n","      <td>DET</td>\n","      <td>17</td>\n","      <td>8</td>\n","      <td>21.6</td>\n","      <td>2.1</td>\n","      <td>4.2</td>\n","      <td>0.507</td>\n","      <td>...</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>F</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>2.95977</td>\n","      <td>12.598037</td>\n","      <td>0.470588</td>\n","      <td>0.470588</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Jared Butler</td>\n","      <td>SG</td>\n","      <td>23</td>\n","      <td>WAS</td>\n","      <td>40</td>\n","      <td>0</td>\n","      <td>14.2</td>\n","      <td>2.5</td>\n","      <td>5.0</td>\n","      <td>0.488</td>\n","      <td>...</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>G</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>18.09782</td>\n","      <td>14.408717</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Grayson Allen</td>\n","      <td>SG</td>\n","      <td>28</td>\n","      <td>PHO</td>\n","      <td>75</td>\n","      <td>74</td>\n","      <td>33.5</td>\n","      <td>4.5</td>\n","      <td>9.1</td>\n","      <td>0.499</td>\n","      <td>...</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>G</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>89.25000</td>\n","      <td>16.004367</td>\n","      <td>0.986667</td>\n","      <td>0.986667</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Daniel Theis</td>\n","      <td>C</td>\n","      <td>31</td>\n","      <td>LAC</td>\n","      <td>60</td>\n","      <td>3</td>\n","      <td>16.9</td>\n","      <td>2.6</td>\n","      <td>4.9</td>\n","      <td>0.532</td>\n","      <td>...</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>C</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>91.08387</td>\n","      <td>16.024706</td>\n","      <td>0.050000</td>\n","      <td>0.050000</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>489</th>\n","      <td>Kevin Knox</td>\n","      <td>SF</td>\n","      <td>24</td>\n","      <td>DET</td>\n","      <td>31</td>\n","      <td>11</td>\n","      <td>18.1</td>\n","      <td>2.8</td>\n","      <td>6.0</td>\n","      <td>0.462</td>\n","      <td>...</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>F</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>21.44320</td>\n","      <td>14.578333</td>\n","      <td>0.354839</td>\n","      <td>0.354839</td>\n","    </tr>\n","    <tr>\n","      <th>490</th>\n","      <td>KJ Martin</td>\n","      <td>SF</td>\n","      <td>23</td>\n","      <td>PHI</td>\n","      <td>60</td>\n","      <td>2</td>\n","      <td>12.4</td>\n","      <td>1.6</td>\n","      <td>3.0</td>\n","      <td>0.536</td>\n","      <td>...</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>F</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>19.30681</td>\n","      <td>14.473383</td>\n","      <td>0.033333</td>\n","      <td>0.033333</td>\n","    </tr>\n","    <tr>\n","      <th>491</th>\n","      <td>Matt Ryan</td>\n","      <td>SF</td>\n","      <td>26</td>\n","      <td>NOP</td>\n","      <td>28</td>\n","      <td>1</td>\n","      <td>13.9</td>\n","      <td>1.8</td>\n","      <td>4.0</td>\n","      <td>0.434</td>\n","      <td>...</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>F</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>20.59782</td>\n","      <td>14.538111</td>\n","      <td>0.035714</td>\n","      <td>0.035714</td>\n","    </tr>\n","    <tr>\n","      <th>492</th>\n","      <td>JD Davison</td>\n","      <td>SG</td>\n","      <td>21</td>\n","      <td>BOS</td>\n","      <td>8</td>\n","      <td>0</td>\n","      <td>4.9</td>\n","      <td>0.6</td>\n","      <td>1.5</td>\n","      <td>0.417</td>\n","      <td>...</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>G</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>5.59782</td>\n","      <td>13.235303</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>493</th>\n","      <td>Jack White</td>\n","      <td>SF</td>\n","      <td>26</td>\n","      <td>MEM</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>16.0</td>\n","      <td>0.5</td>\n","      <td>4.0</td>\n","      <td>0.125</td>\n","      <td>...</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>F</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>7.03550</td>\n","      <td>13.463894</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>494 rows × 60 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8bd4f884-8494-4964-8fa9-ae33dfc3e8e3')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-8bd4f884-8494-4964-8fa9-ae33dfc3e8e3 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-8bd4f884-8494-4964-8fa9-ae33dfc3e8e3');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-528ff0b8-e112-4cb1-9a1c-3e4e9025cfb9\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-528ff0b8-e112-4cb1-9a1c-3e4e9025cfb9')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-528ff0b8-e112-4cb1-9a1c-3e4e9025cfb9 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"nba_stats"}},"metadata":{},"execution_count":11}],"source":["nba_stats = pd.read_csv('./nba_player_stats_v2.csv')\n","nba_stats"]},{"cell_type":"code","execution_count":null,"id":"b2526c6f","metadata":{"id":"b2526c6f"},"outputs":[],"source":["nba_stats = nba_stats.dropna()"]},{"cell_type":"code","execution_count":null,"id":"fa01f8b8","metadata":{"id":"fa01f8b8"},"outputs":[],"source":["y = nba_stats['lgSalary']\n","X = nba_stats.drop(columns = ['Player', 'Pos', 'Age', 'Tm', 'PosCtg', 'lgSalary'])\n","alpha = 0.05\n","\n","draft = LAZY_MLR(y, X, alpha)"]},{"cell_type":"code","execution_count":null,"id":"63e9486d","metadata":{"scrolled":false,"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"15Yf-9X-x9gsdRWxfjrIzwt5IdAqMdH4C"},"id":"63e9486d","executionInfo":{"status":"ok","timestamp":1717826975709,"user_tz":-480,"elapsed":15472,"user":{"displayName":"morris chen","userId":"05337989253267718231"}},"outputId":"a673e300-d175-4d85-84de-3e78a1d04e5f"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["draft.get_scatter_plots()"]},{"cell_type":"code","execution_count":null,"id":"bf5403c5","metadata":{"id":"bf5403c5"},"outputs":[],"source":["draft.best_subset()"]},{"cell_type":"code","source":[],"metadata":{"id":"RafjSD7Dd4_n"},"id":"RafjSD7Dd4_n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"},"colab":{"provenance":[],"machine_shape":"hm","gpuType":"L4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}